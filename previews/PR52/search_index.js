var documenterSearchIndex = {"docs":
[{"location":"api/#","page":"API","title":"API","text":"CurrentModule = NearestNeighborModels","category":"page"},{"location":"api/#API-1","page":"API","title":"API","text":"","category":"section"},{"location":"api/#Models-1","page":"API","title":"Models","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"KNNClassifier\nKNNRegressor\nMultitargetKNNRegressor\nMultitargetKNNClassifier","category":"page"},{"location":"api/#NearestNeighborModels.KNNClassifier","page":"API","title":"NearestNeighborModels.KNNClassifier","text":"KNNClassifier(;kwargs...)\n\nK-Nearest Neighbors classifier: predicts the class associated with a new point by taking a vote over the classes of the K-nearest points.\n\nKeywords Parameters\n\nK::Int=5 : number of neighbors\nalgorithm::Symbol = :kdtree : one of (:kdtree, :brutetree, :balltree)\nmetric::Metric = Euclidean() : any Metric from    Distances.jl for the    distance between points. For algorithm = :kdtree only metrics which are of    type Union{Distances.Chebyshev, Distances.Cityblock, Distances.Euclidean, Distances.Minkowski, Distances.WeightedCityblock, Distances.WeightedEuclidean, Distances.WeightedMinkowski} are supported.\nleafsize::Int = algorithm == 10 : determines the number of points    at which to stop splitting the tree. This option is ignored and always taken as 0    for algorithm = :brutetree, since brutetree isn't actually a tree.\nreorder::Bool = true : if true then points which are close in    distance are placed close in memory. In this case, a copy of the original data    will be made so that the original data is left unmodified. Setting this to true    can significantly improve performance of the specified algorithm    (except :brutetree). This option is ignored and always taken as false for    algorithm = :brutetree.\nweights::KNNKernel=Uniform() : kernel used in assigning weights to the    k-nearest neighbors for each observation. An instance of one of the types in    list_kernels(). User-defined weighting functions can be passed by wrapping the    function in a UDF kernel. If sample weights w are passed during machine    construction e.g machine(model, X, y, w) then the weight assigned to each    neighbor is the product of the KNNKernel generated weight and the corresponding    neighbor sample weight.\n\nSee also the  package documentation. For more information about the kernels see the paper by Geler et.al  Comparison of different weighting schemes for the kNN classifier on time-series data.\n\n\n\n\n\n","category":"type"},{"location":"api/#NearestNeighborModels.KNNRegressor","page":"API","title":"NearestNeighborModels.KNNRegressor","text":"KNNRegressor(;kwargs...)\n\nK-Nearest Neighbors regressor: predicts the response associated with a new point by taking an weighted average of the response of the K-nearest points.\n\nKeywords Parameters\n\nK::Int=5 : number of neighbors\nalgorithm::Symbol = :kdtree : one of (:kdtree, :brutetree, :balltree)\nmetric::Metric = Euclidean() : any Metric from    Distances.jl for the    distance between points. For algorithm = :kdtree only metrics which are of    type Union{Distances.Chebyshev, Distances.Cityblock, Distances.Euclidean, Distances.Minkowski, Distances.WeightedCityblock, Distances.WeightedEuclidean, Distances.WeightedMinkowski} are supported.\nleafsize::Int = algorithm == 10 : determines the number of points    at which to stop splitting the tree. This option is ignored and always taken as 0    for algorithm = :brutetree, since brutetree isn't actually a tree.\nreorder::Bool = true : if true then points which are close in    distance are placed close in memory. In this case, a copy of the original data    will be made so that the original data is left unmodified. Setting this to true    can significantly improve performance of the specified algorithm    (except :brutetree). This option is ignored and always taken as false for    algorithm = :brutetree.\nweights::KNNKernel=Uniform() : kernel used in assigning weights to the    k-nearest neighbors for each observation. An instance of one of the types in    list_kernels(). User-defined weighting functions can be passed by wrapping the    function in a UDF kernel. If sample weights w are passed during machine    construction e.g machine(model, X, y, w) then the weight assigned to each    neighbor is the product of the KNNKernel generated weight and the corresponding    neighbor sample weight.\n\nSee also the  package documentation. For more information about the kernels see the paper by Geler et.al  Comparison of different weighting schemes for the kNN classifier on time-series data.\n\n\n\n\n\n","category":"type"},{"location":"api/#NearestNeighborModels.MultitargetKNNRegressor","page":"API","title":"NearestNeighborModels.MultitargetKNNRegressor","text":"MultitargetKNNRegressor(;kwargs...)\n\nK-Nearest Neighbors regressor: predicts the response associated with a new point by taking an weighted average of the response of the K-nearest points.\n\nKeywords Parameters\n\nK::Int=5 : number of neighbors\nalgorithm::Symbol = :kdtree : one of (:kdtree, :brutetree, :balltree)\nmetric::Metric = Euclidean() : any Metric from    Distances.jl for the    distance between points. For algorithm = :kdtree only metrics which are of    type Union{Distances.Chebyshev, Distances.Cityblock, Distances.Euclidean, Distances.Minkowski, Distances.WeightedCityblock, Distances.WeightedEuclidean, Distances.WeightedMinkowski} are supported.\nleafsize::Int = algorithm == 10 : determines the number of points    at which to stop splitting the tree. This option is ignored and always taken as 0    for algorithm = :brutetree, since brutetree isn't actually a tree.\nreorder::Bool = true : if true then points which are close in    distance are placed close in memory. In this case, a copy of the original data    will be made so that the original data is left unmodified. Setting this to true    can significantly improve performance of the specified algorithm    (except :brutetree). This option is ignored and always taken as false for    algorithm = :brutetree.\nweights::KNNKernel=Uniform() : kernel used in assigning weights to the    k-nearest neighbors for each observation. An instance of one of the types in    list_kernels(). User-defined weighting functions can be passed by wrapping the    function in a UDF kernel. If sample weights w are passed during machine    construction e.g machine(model, X, y, w) then the weight assigned to each    neighbor is the product of the KNNKernel generated weight and the corresponding    neighbor sample weight.\n\nSee also the  package documentation. For more information about the kernels see the paper by Geler et.al  Comparison of different weighting schemes for the kNN classifier on time-series data.\n\n\n\n\n\n","category":"type"},{"location":"api/#NearestNeighborModels.MultitargetKNNClassifier","page":"API","title":"NearestNeighborModels.MultitargetKNNClassifier","text":"MultitargetKNNClassifier(;kwargs...)\n\nK-Nearest Neighbors classifier: predicts the class associated with a new point by taking a vote over the classes of the K-nearest points.\n\nKeywords Parameters\n\nK::Int=5 : number of neighbors\nalgorithm::Symbol = :kdtree : one of (:kdtree, :brutetree, :balltree)\nmetric::Metric = Euclidean() : any Metric from    Distances.jl for the    distance between points. For algorithm = :kdtree only metrics which are of    type Union{Distances.Chebyshev, Distances.Cityblock, Distances.Euclidean, Distances.Minkowski, Distances.WeightedCityblock, Distances.WeightedEuclidean, Distances.WeightedMinkowski} are supported.\nleafsize::Int = algorithm == 10 : determines the number of points    at which to stop splitting the tree. This option is ignored and always taken as 0    for algorithm = :brutetree, since brutetree isn't actually a tree.\nreorder::Bool = true : if true then points which are close in    distance are placed close in memory. In this case, a copy of the original data    will be made so that the original data is left unmodified. Setting this to true    can significantly improve performance of the specified algorithm    (except :brutetree). This option is ignored and always taken as false for    algorithm = :brutetree.\nweights::KNNKernel=Uniform() : kernel used in assigning weights to the    k-nearest neighbors for each observation. An instance of one of the types in    list_kernels(). User-defined weighting functions can be passed by wrapping the    function in a UDF kernel. If sample weights w are passed during machine    construction e.g machine(model, X, y, w) then the weight assigned to each    neighbor is the product of the KNNKernel generated weight and the corresponding    neighbor sample weight.\n\noutput_type::Type{<:MultiUnivariateFinite}=DictTable : One of   (ColumnTable, DictTable). The type of table type to use for predictions.  Setting to ColumnTable might improve performance for narrow tables while setting to   DictTable improves performance for wide tables.\n\nSee also the  package documentation. For more information about the kernels see the paper by Geler et.al  Comparison of different weighting schemes for the kNN classifier on time-series data.\n\n\n\n\n\n","category":"type"},{"location":"api/#Kernels-1","page":"API","title":"Kernels","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"KNNKernel\nlist_kernels\nDualU\nDualD\nDudani\nFibonacci\nInverse\nISquared\nMacleod\nRank\nReciprocalRank\nUDK\nUniform\nUserDefinedKernel\nZavreal","category":"page"},{"location":"api/#NearestNeighborModels.KNNKernel","page":"API","title":"NearestNeighborModels.KNNKernel","text":"KNNKernel\n\nAbstract super type for all weighting kernels\n\n\n\n\n\n","category":"type"},{"location":"api/#NearestNeighborModels.list_kernels","page":"API","title":"NearestNeighborModels.list_kernels","text":"list_kernels()\n\nLists all implemented KNN weighting kernels\n\n\n\n\n\n","category":"function"},{"location":"api/#NearestNeighborModels.DualU","page":"API","title":"NearestNeighborModels.DualU","text":"DualU()\n\nAssigns the closest neighbor a weight of 1, the furthest neighbor weight 0 and the  others are scaled between by a mapping.\n\nFor more information see the paper by Geler et.al Comparison of different weighting  schemes for the kNN classifier on time-series data.\n\nsee also: DualD\n\n\n\n\n\n","category":"type"},{"location":"api/#NearestNeighborModels.DualD","page":"API","title":"NearestNeighborModels.DualD","text":"DualD()\n\nAssigns the closest neighbor a weight of 1, the furthest neighbor weight 0 and the  others are scaled between by a mapping.\n\nFor more information see the paper by Geler et.al Comparison of different weighting  schemes for the kNN classifier on time-series data.\n\nsee also: DualU\n\n\n\n\n\n","category":"type"},{"location":"api/#NearestNeighborModels.Dudani","page":"API","title":"NearestNeighborModels.Dudani","text":"Dudani()\n\nAssigns the closest neighbor a weight of 1, the furthest neighbor weight 0 and the  others are scaled between by a linear mapping.\n\nFor more information see the paper by Geler et.al Comparison of different weighting  schemes for the kNN classifier on time-series data.\n\nsee also: Macleod, DualU, DualD\n\n\n\n\n\n","category":"type"},{"location":"api/#NearestNeighborModels.Fibonacci","page":"API","title":"NearestNeighborModels.Fibonacci","text":"Fibonacci()\n\nAssigns neighbors weights corresponding to fibonacci numbers starting from the furthest neighbor. i.e the furthest neighbor a weight of 1, the second furthest neighbor a  weight of 1 and the third furthest neighbor a weight of 2 and so on.  \n\nFor more information see the paper by Geler et.al Comparison of different weighting  schemes for the kNN classifier on time-series data.\n\n\n\n\n\n","category":"type"},{"location":"api/#NearestNeighborModels.Inverse","page":"API","title":"NearestNeighborModels.Inverse","text":"Inverse()\n\nAssigns each neighbor a weight equal to the inverse of the corresponsting distance of the  neighbor.\n\nFor more information see the paper by Geler et.al Comparison of different weighting  schemes for the kNN classifier on time-series data.\n\nsee also: ISquared\n\n\n\n\n\n","category":"type"},{"location":"api/#NearestNeighborModels.ISquared","page":"API","title":"NearestNeighborModels.ISquared","text":"ISquared()\n\nAssigns each neighbor a weight equal to the inverse of the corresponsting squared-distance  of the neighbor.\n\nFor more information see the paper by Geler et.al Comparison of different weighting  schemes for the kNN classifier on time-series data.\n\n\n\n\n\n","category":"type"},{"location":"api/#NearestNeighborModels.Macleod","page":"API","title":"NearestNeighborModels.Macleod","text":"Macleod(;a::Real= 0.0)\n\nAssigns the closest neighbor a weight of 1, the furthest neighbor weight 0 and the  others are scaled between by a linear mapping.\n\nFor more information see the paper by Geler et.al Comparison of different weighting  schemes for the kNN classifier on time-series data.\n\nsee also: Dudani, DualU, DualD\n\n\n\n\n\n","category":"type"},{"location":"api/#NearestNeighborModels.Rank","page":"API","title":"NearestNeighborModels.Rank","text":"Rank()\n\nAssigns each neighbor a weight as a rank such that the closest neighbor get's a weight of  1 and the Kth closest neighbor gets a weight of K.\n\nFor more information see the paper by Geler et.al Comparison of different weighting  schemes for the kNN classifier on time-series data.\n\nsee also: ReciprocalRank\n\n\n\n\n\n","category":"type"},{"location":"api/#NearestNeighborModels.ReciprocalRank","page":"API","title":"NearestNeighborModels.ReciprocalRank","text":"ReciprocalRank(;a::Real= 0.0)\n\nAssigns each closest neighbor a weight which is equal to the reciprocal of it's rank.  i.e the closest neighbor get's a weight of 1 and the Kth closest weight get's a weight  of 1/K \n\nFor more information see the paper by Geler et.al Comparison of different weighting  schemes for the kNN classifier on time-series data.\n\nsee also: Rank\n\n\n\n\n\n","category":"type"},{"location":"api/#NearestNeighborModels.UDK","page":"API","title":"NearestNeighborModels.UDK","text":"UDK\n\nAlias for UserDefinedKernel\n\n\n\n\n\n","category":"type"},{"location":"api/#NearestNeighborModels.Uniform","page":"API","title":"NearestNeighborModels.Uniform","text":"Uniform()\n\nassigns uniform weights to all k-nearest neighbors.\n\nsee also: Inverse, ISquared, Zavreal     \n\n\n\n\n\n","category":"type"},{"location":"api/#NearestNeighborModels.UserDefinedKernel","page":"API","title":"NearestNeighborModels.UserDefinedKernel","text":"UserDefinedKernel(;func::Function = x->nothing, sort::Bool=false)\n\nWrap a user defined nearest neighbors weighting function func as a KNNKernel.\n\nKeywords\n\nfunc : user-defined nearest neighbors weighting function. The function   should have the signature func(dists_matrix)::Union{Nothing, <:AbstractMatrix}.  The dists_matrix is a n by K nearest neighbors distances matrix where   n is the number of samples in the test dataset and K is number of neighbors.   func should either output nothing or an AbstractMatrix of the same shape   as dists_matrix.  If func(dists_matrix) returns nothing then all k-nearest   neighbors in each row are assign equal weights.\nsort : if true requests that the dists_matrix be sorted before being passed   to func. The sort is done in a manner that puts the k-nearest neighbors in   each row  of dists_matrix in acesending order .\n\n\n\n\n\n","category":"type"},{"location":"api/#NearestNeighborModels.Zavreal","page":"API","title":"NearestNeighborModels.Zavreal","text":"Zavreal(;s::Real = 0.0, a::Real=1.0)\n\nAssigns each neighbor an exponential weight given by     e^ - α  d_i^eta where α and β are constants and dᵢ is the distance of the given neighbor.\n\nFor more information see the paper by Geler et.al Comparison of different weighting  schemes for the kNN classifier on time-series data.\n\n\n\n\n\n","category":"type"},{"location":"#NearestNeighborModels-Docs-1","page":"Home","title":"NearestNeighborModels - Docs","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"NearestNeighborModels is a julia package providing implemtation of various  k-nearest-neighbor classifiers and regressors models for use with  MLJ machine learning  framework. It also provides users with an array of weighting kernels to choose  from for prediction.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"NearestNeighborModels builds on Kristoffer Carlsson's  NearestNeighbors package(for  performing efficient nearest neighbor searches) and earlier contributions from Thibaut  Lienart originally residing in  MLJModels.jl.","category":"page"},{"location":"#Installation-1","page":"Home","title":"Installation","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"On a Julia>=1.0 NearestNeighborModels can be added via Pkg as shown below.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"NearestNeighborModels\") ","category":"page"},{"location":"#Usage-1","page":"Home","title":"Usage","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"To use any model implemented in this package, the model must first be wrapped in an MLJ  machine alongside the required data. Users also get additional features from MLJ including  performance evaluation, hyper-parameter tuning, stacking etc. The following example shows how to train a KNNClassifier on the crabs dataset.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"using NearestNeighborModels, MLJBase\nX, y = @load_crabs; # loads the crabs dataset from MLJBase\ntrain_inds, test_inds = partition(1:nrows(X), 0.7, shuffle=false);\nknnc = KNNClassifier(weights = Inverse()) # KNNClassifier instantiation\nknnc_mach = machine(knnc, X, y) # wrap model and required data in an MLJ machine\nfit!(knnc_mach, rows=train_inds) # train machine on a subset of the wrapped data `X`","category":"page"},{"location":"#","page":"Home","title":"Home","text":"UnivariateFinite predictions can be obtained from the trained machine as shown below","category":"page"},{"location":"#","page":"Home","title":"Home","text":"DocTestSetup = quote\n    using NearestNeighborModels, MLJBase\n    X, y = @load_crabs;\n    train_inds, test_inds = partition(1:nrows(X), 0.7, shuffle=false);\n    knnc = KNNClassifier(weights = Inverse())\n    knnc_mach = machine(knnc, X, y)\n    fit!(knnc_mach, rows=train_inds)\nend","category":"page"},{"location":"#","page":"Home","title":"Home","text":"julia> predict(knnc_mach, rows=test_inds)\n60-element UnivariateFiniteVector{Multiclass{2}, String, UInt32, Float64}:\n UnivariateFinite{Multiclass{2}}(B=>0.315, O=>0.685)\n UnivariateFinite{Multiclass{2}}(B=>1.0, O=>0.0)\n UnivariateFinite{Multiclass{2}}(B=>1.0, O=>0.0)\n UnivariateFinite{Multiclass{2}}(B=>1.0, O=>0.0)\n UnivariateFinite{Multiclass{2}}(B=>1.0, O=>0.0)\n UnivariateFinite{Multiclass{2}}(B=>1.0, O=>0.0)\n UnivariateFinite{Multiclass{2}}(B=>1.0, O=>0.0)\n UnivariateFinite{Multiclass{2}}(B=>1.0, O=>0.0)\n UnivariateFinite{Multiclass{2}}(B=>1.0, O=>0.0)\n UnivariateFinite{Multiclass{2}}(B=>1.0, O=>0.0)\n ⋮\n UnivariateFinite{Multiclass{2}}(B=>0.613, O=>0.387)\n UnivariateFinite{Multiclass{2}}(B=>0.83, O=>0.17)\n UnivariateFinite{Multiclass{2}}(B=>0.361, O=>0.639)\n UnivariateFinite{Multiclass{2}}(B=>1.0, O=>0.0)\n UnivariateFinite{Multiclass{2}}(B=>0.824, O=>0.176)\n UnivariateFinite{Multiclass{2}}(B=>1.0, O=>0.0)\n UnivariateFinite{Multiclass{2}}(B=>1.0, O=>0.0)\n UnivariateFinite{Multiclass{2}}(B=>1.0, O=>0.0)\n UnivariateFinite{Multiclass{2}}(B=>1.0, O=>0.0)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Alternatively categorical predictions may be obtained using predict_mode as shown below.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"julia> predict_mode(knnc_mach, rows=test_inds)\n60-element CategoricalArrays.CategoricalArray{String,1,UInt32}:\n \"O\"\n \"B\"\n \"B\"\n \"B\"\n \"B\"\n \"B\"\n \"B\"\n \"B\"\n \"B\"\n \"B\"\n ⋮\n \"B\"\n \"B\"\n \"O\"\n \"B\"\n \"B\"\n \"B\"\n \"B\"\n \"B\"\n \"B\"\n","category":"page"},{"location":"#","page":"Home","title":"Home","text":"DocTestSetup = nothing","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Users who need to implement their weighting kernel may do so by wrapping the kernel function in a UserDefinedKernel. The following code shows how to define a UserDefinedKernel that assigns weights in such a way that the closest neighbor in each row of dists is assigned a weight of 2 while other neighbors in the same row are assigned equal weights of 1.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"# First we define the kernel function\nfunction custom_kernel(dists::AbstractMatrix)\n    # eltype of weights must be `<:AbstractFloat`\n    weights = similar(Array{Float16}, size(dists))\n    weights[:, 1] .= 2.0\n    weights[:, 2:end] .= 1.0\n    return weights\nend\n\n# Then we wrap it in a `UserDefinedKernel`\n# `sort = true` because our `custom_kernel` function relies on `dists` being sorted in \n# ascending order.\nweighting_kernel = UserDefinedKernel(func=custom_kernel, sort=true)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"We will now train a MultitargetKNNRegressor that makes use of our simple custom-defined  weighting_kernel for prediction.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"using NearestNeighborModels, MLJBase\nusing StableRNGs #for reproducibility of this example\n\nn = 50\np = 5\nl = 2\nrng = StableRNG(100)\n# `table` converts an `AbstractMatrix` into a `Tables.jl` compactible table\nX = table(randn(rng, (n, p))) # feature table\nY = table(randn(rng, (n, l))) # target table\n\ntrain_inds, test_inds = partition(1:nrows(X), 0.8, shuffle=false);\nmulti_knnr = MultitargetKNNRegressor(weights=weighting_kernel)\nmulti_knnr_mach = machine(multi_knnr, X, Y) #wrap model and required data in an MLJ machine\nfit!(multi_knnr_mach, rows=train_inds) # train machine on a subset of the wrapped data `X`","category":"page"},{"location":"#","page":"Home","title":"Home","text":"And of course predicting with the test-dataset gives:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"DocTestSetup = quote\n    using NearestNeighborModels, MLJBase, StableRNGs\n    function custom_kernel(dists::AbstractMatrix)\n        weights = similar(Array{Float16}, size(dists))\n        weights[:, 1] .= 2.0\n        weights[:, 2:end] .= 1.0\n        return weights\n    end\n    weighting_kernel = UserDefinedKernel(func=custom_kernel, sort=true)\n    n = 50\n    p = 5\n    l = 2\n    rng = StableRNG(100)\n    X = table(randn(rng, (n, p))) # feature table\n    Y = table(randn(rng, (n, l))) # target table\n    train_inds, test_inds = partition(1:nrows(X), 0.8, shuffle=false);\n    multi_knnr = MultitargetKNNRegressor(weights=weighting_kernel)\n    multi_knnr_mach = machine(multi_knnr, X, Y) #wrap model and required data in an MLJ machine\n    fit!(multi_knnr_mach, rows=train_inds) # train machine on a subset of the wrapped data `X`\nend","category":"page"},{"location":"#","page":"Home","title":"Home","text":"julia> table_predictions = predict(multi_knnr_mach, rows=test_inds)\nTables.MatrixTable{Matrix{Float64}} with 10 rows, 2 columns, and schema:\n :x1  Float64\n :x2  Float64\n\njulia> MLJBase.matrix(table_predictions)\n10×2 Matrix{Float64}:\n  0.0867655   0.286866\n  0.388722    0.11164\n -0.0572281   0.752707\n -0.0845422   0.125039\n  0.308204   -0.0840635\n  0.170179   -0.0136838\n  0.0916878   0.168172\n  0.909645   -0.788285\n -0.35767     0.158968\n  0.0352492  -0.30166","category":"page"},{"location":"#","page":"Home","title":"Home","text":"DocTestSetup = nothing","category":"page"},{"location":"#","page":"Home","title":"Home","text":"see MLJ docs for help on additional  features such as hyper-parameter tuning, performance evaluation, stacking etc.","category":"page"}]
}
