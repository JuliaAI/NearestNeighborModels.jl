<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API · NearestNeighborModels.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">NearestNeighborModels.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>API</a><ul class="internal"><li class="toplevel"><a class="tocitem" href="#Models-1"><span>Models</span></a></li><li class="toplevel"><a class="tocitem" href="#Kernels-1"><span>Kernels</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaAI/NearestNeighborModels.jl/blob/master/docs/src/api.md#L" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="API-1"><a class="docs-heading-anchor" href="#API-1">API</a><a class="docs-heading-anchor-permalink" href="#API-1" title="Permalink"></a></h1><h1 id="Models-1"><a class="docs-heading-anchor" href="#Models-1">Models</a><a class="docs-heading-anchor-permalink" href="#Models-1" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="NearestNeighborModels.KNNClassifier" href="#NearestNeighborModels.KNNClassifier"><code>NearestNeighborModels.KNNClassifier</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">KNNClassifier</code></pre><p>A model type for constructing a K-nearest neighbor classifier, based on <a href="https://github.com/JuliaAI/NearestNeighborModels.jl">NearestNeighborModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="language-none">KNNClassifier = @load KNNClassifier pkg=NearestNeighborModels</code></pre><p>Do <code>model = KNNClassifier()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>KNNClassifier(K=...)</code>.</p><p>KNNClassifier implements <a href="https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm">K-Nearest Neighbors classifier</a>  which is non-parametric algorithm that predicts a discrete class distribution associated  with a new point by taking a vote over the classes of the k-nearest points. Each neighbor  vote is assigned a weight based on proximity of the neighbor point to the test point  according to a specified distance metric.</p><p>For more information about the weighting kernels, see the paper by Geler et.al  <a href="https://perun.pmf.uns.ac.rs/radovanovic/publications/2016-kais-knn-weighting.pdf">Comparison of different weighting schemes for the kNN classifier on time-series data</a>. </p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="language-none">mach = machine(model, X, y)</code></pre><p>OR</p><pre><code class="language-none">mach = machine(model, X, y, w)</code></pre><p>Here:</p><ul><li><p><code>X</code> is any table of input features (eg, a <code>DataFrame</code>) whose columns are of scitype <code>Continuous</code>; check column scitypes with <code>schema(X)</code>.</p></li><li><p><code>y</code> is the target, which can be any <code>AbstractVector</code> whose element scitype is <code>&lt;:Finite</code> (<code>&lt;:Multiclass</code> or <code>&lt;:OrderedFactor</code> will do); check the scitype with <code>scitype(y)</code></p></li><li><p><code>w</code> is the observation weights which can either be <code>nothing</code> (default) or an  <code>AbstractVector</code> whose element scitype is <code>Count</code> or <code>Continuous</code>. This is  different from <code>weights</code> kernel which is a model hyperparameter, see below.</p></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>K::Int=5</code> : number of neighbors</li><li><code>algorithm::Symbol = :kdtree</code> : one of <code>(:kdtree, :brutetree, :balltree)</code></li><li><code>metric::Metric = Euclidean()</code> : any <code>Metric</code> from    <a href="https://github.com/JuliaStats/Distances.jl">Distances.jl</a> for the    distance between points. For <code>algorithm = :kdtree</code> only metrics which are    instances of <code>Union{Distances.Chebyshev, Distances.Cityblock, Distances.Euclidean, Distances.Minkowski, Distances.WeightedCityblock, Distances.WeightedEuclidean, Distances.WeightedMinkowski}</code> are supported.</li><li><code>leafsize::Int = algorithm == 10</code> : determines the number of points    at which to stop splitting the tree. This option is ignored and always taken as <code>0</code>    for <code>algorithm = :brutetree</code>, since <code>brutetree</code> isn&#39;t actually a tree.</li><li><code>reorder::Bool = true</code> : if <code>true</code> then points which are close in    distance are placed close in memory. In this case, a copy of the original data    will be made so that the original data is left unmodified. Setting this to <code>true</code>    can significantly improve performance of the specified <code>algorithm</code>    (except <code>:brutetree</code>). This option is ignored and always taken as <code>false</code> for    <code>algorithm = :brutetree</code>.</li><li><code>weights::KNNKernel=Uniform()</code> : kernel used in assigning weights to the    k-nearest neighbors for each observation. An instance of one of the types in    <code>list_kernels()</code>. User-defined weighting functions can be passed by wrapping the    function in a <a href="#NearestNeighborModels.UserDefinedKernel"><code>UserDefinedKernel</code></a> kernel (do <code>?NearestNeighborModels.UserDefinedKernel</code> for more    info). If observation weights <code>w</code> are passed during machine construction then the    weight assigned to each neighbor vote is the product of the kernel generated    weight for that neighbor and the corresponding observation weight.</li></ul><p><strong>Operations</strong></p><ul><li><p><code>predict(mach, Xnew)</code>: Return predictions of the target given features <code>Xnew</code>, which should have same scitype as <code>X</code> above. Predictions are probabilistic but uncalibrated.</p></li><li><p><code>predict_mode(mach, Xnew)</code>: Return the modes of the probabilistic predictions returned above.</p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>tree</code>: An instance of either <code>KDTree</code>, <code>BruteTree</code> or <code>BallTree</code> depending on the  value of the <code>algorithm</code> hyperparameter (See hyper-parameters section above).  These are data structures that stores the training data with the view of making  quicker nearest neighbor searches on test data points.</li></ul><p><strong>Examples</strong></p><pre><code class="language-none">using MLJ
KNNClassifier = @load KNNClassifier pkg=NearestNeighborModels
X, y = @load_crabs; # a table and a vector from the crabs dataset
# view possible kernels
NearestNeighborModels.list_kernels()
# KNNClassifier instantiation
model = KNNClassifier(weights = NearestNeighborModels.Inverse())
mach = machine(model, X, y) |&gt; fit! # wrap model and required data in an MLJ machine and fit
y_hat = predict(mach, X)
labels = predict_mode(mach, X)
</code></pre><p>See also <a href="#NearestNeighborModels.MultitargetKNNClassifier"><code>MultitargetKNNClassifier</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/NearestNeighborModels.jl/blob/0664bab3ab3e6a09a10af344c745a7da48c2a977/src/models.jl#LL536-L580">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NearestNeighborModels.KNNRegressor" href="#NearestNeighborModels.KNNRegressor"><code>NearestNeighborModels.KNNRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">KNNRegressor</code></pre><p>A model type for constructing a K-nearest neighbor regressor, based on <a href="https://github.com/JuliaAI/NearestNeighborModels.jl">NearestNeighborModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="language-none">KNNRegressor = @load KNNRegressor pkg=NearestNeighborModels</code></pre><p>Do <code>model = KNNRegressor()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>KNNRegressor(K=...)</code>.</p><p>KNNRegressor implements <a href="https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm">K-Nearest Neighbors regressor</a>  which is non-parametric algorithm that predicts the response associated with a new point  by taking an weighted average of the response of the K-nearest points.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="language-none">mach = machine(model, X, y)</code></pre><p>OR</p><pre><code class="language-none">mach = machine(model, X, y, w)</code></pre><p>Here:</p><ul><li><p><code>X</code> is any table of input features (eg, a <code>DataFrame</code>) whose columns are of scitype <code>Continuous</code>; check column scitypes with <code>schema(X)</code>.</p></li><li><p><code>y</code> is the target, which can be any table of responses whose element scitype is    <code>Continuous</code>; check the scitype with <code>scitype(y)</code>.</p></li><li><p><code>w</code> is the observation weights which can either be <code>nothing</code>(default) or an  <code>AbstractVector</code> whoose element scitype is <code>Count</code> or <code>Continuous</code>. This is different  from <code>weights</code> kernel which is an hyperparameter to the model, see below.</p></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>K::Int=5</code> : number of neighbors</li><li><code>algorithm::Symbol = :kdtree</code> : one of <code>(:kdtree, :brutetree, :balltree)</code></li><li><code>metric::Metric = Euclidean()</code> : any <code>Metric</code> from    <a href="https://github.com/JuliaStats/Distances.jl">Distances.jl</a> for the    distance between points. For <code>algorithm = :kdtree</code> only metrics which are    instances of <code>Union{Distances.Chebyshev, Distances.Cityblock, Distances.Euclidean, Distances.Minkowski, Distances.WeightedCityblock, Distances.WeightedEuclidean, Distances.WeightedMinkowski}</code> are supported.</li><li><code>leafsize::Int = algorithm == 10</code> : determines the number of points    at which to stop splitting the tree. This option is ignored and always taken as <code>0</code>    for <code>algorithm = :brutetree</code>, since <code>brutetree</code> isn&#39;t actually a tree.</li><li><code>reorder::Bool = true</code> : if <code>true</code> then points which are close in    distance are placed close in memory. In this case, a copy of the original data    will be made so that the original data is left unmodified. Setting this to <code>true</code>    can significantly improve performance of the specified <code>algorithm</code>    (except <code>:brutetree</code>). This option is ignored and always taken as <code>false</code> for    <code>algorithm = :brutetree</code>.</li><li><code>weights::KNNKernel=Uniform()</code> : kernel used in assigning weights to the    k-nearest neighbors for each observation. An instance of one of the types in    <code>list_kernels()</code>. User-defined weighting functions can be passed by wrapping the    function in a <a href="#NearestNeighborModels.UserDefinedKernel"><code>UserDefinedKernel</code></a> kernel (do <code>?NearestNeighborModels.UserDefinedKernel</code> for more    info). If observation weights <code>w</code> are passed during machine construction then the    weight assigned to each neighbor vote is the product of the kernel generated    weight for that neighbor and the corresponding observation weight.</li></ul><p><strong>Operations</strong></p><ul><li><code>predict(mach, Xnew)</code>: Return predictions of the target given features <code>Xnew</code>, which should have same scitype as <code>X</code> above.</li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>tree</code>: An instance of either <code>KDTree</code>, <code>BruteTree</code> or <code>BallTree</code> depending on the  value of the <code>algorithm</code> hyperparameter (See hyper-parameters section above).  These are data structures that stores the training data with the view of making  quicker nearest neighbor searches on test data points.</li></ul><p><strong>Examples</strong></p><pre><code class="language-none">using MLJ
KNNRegressor = @load KNNRegressor pkg=NearestNeighborModels
X, y = @load_boston; # loads the crabs dataset from MLJBase
# view possible kernels
NearestNeighborModels.list_kernels()
model = KNNRegressor(weights = NearestNeighborModels.Inverse()) #KNNRegressor instantiation
mach = machine(model, X, y) |&gt; fit! # wrap model and required data in an MLJ machine and fit
y_hat = predict(mach, X)
</code></pre><p>See also <a href="#NearestNeighborModels.MultitargetKNNRegressor"><code>MultitargetKNNRegressor</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/NearestNeighborModels.jl/blob/0664bab3ab3e6a09a10af344c745a7da48c2a977/src/models.jl#LL697-L741">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NearestNeighborModels.MultitargetKNNRegressor" href="#NearestNeighborModels.MultitargetKNNRegressor"><code>NearestNeighborModels.MultitargetKNNRegressor</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MultitargetKNNRegressor</code></pre><p>A model type for constructing a multitarget K-nearest neighbor regressor, based on <a href="https://github.com/JuliaAI/NearestNeighborModels.jl">NearestNeighborModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="language-none">MultitargetKNNRegressor = @load MultitargetKNNRegressor pkg=NearestNeighborModels</code></pre><p>Do <code>model = MultitargetKNNRegressor()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>MultitargetKNNRegressor(K=...)</code>.</p><p>Multi-target K-Nearest Neighbors regressor (MultitargetKNNRegressor) is a variation of  <a href="#NearestNeighborModels.KNNRegressor"><code>KNNRegressor</code></a> that assumes the target variable is vector-valued with <code>Continuous</code> components. (Target data must be presented as a table, however.)</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="language-none">mach = machine(model, X, y)</code></pre><p>OR</p><pre><code class="language-none">mach = machine(model, X, y, w)</code></pre><p>Here:</p><ul><li><p><code>X</code> is any table of input features (eg, a <code>DataFrame</code>) whose columns are of scitype <code>Continuous</code>; check column scitypes with <code>schema(X)</code>.</p></li><li><p><code>y</code> is the target, which can be any table of responses whose element scitype is  <code>Continuous</code>; check column scitypes with <code>schema(y)</code>.</p></li><li><p><code>w</code> is the observation weights which can either be <code>nothing</code>(default) or an  <code>AbstractVector</code> whoose element scitype is <code>Count</code> or <code>Continuous</code>. This is different  from <code>weights</code> kernel which is an hyperparameter to the model, see below.</p></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>K::Int=5</code> : number of neighbors</li><li><code>algorithm::Symbol = :kdtree</code> : one of <code>(:kdtree, :brutetree, :balltree)</code></li><li><code>metric::Metric = Euclidean()</code> : any <code>Metric</code> from    <a href="https://github.com/JuliaStats/Distances.jl">Distances.jl</a> for the    distance between points. For <code>algorithm = :kdtree</code> only metrics which are    instances of <code>Union{Distances.Chebyshev, Distances.Cityblock, Distances.Euclidean, Distances.Minkowski, Distances.WeightedCityblock, Distances.WeightedEuclidean, Distances.WeightedMinkowski}</code> are supported.</li><li><code>leafsize::Int = algorithm == 10</code> : determines the number of points    at which to stop splitting the tree. This option is ignored and always taken as <code>0</code>    for <code>algorithm = :brutetree</code>, since <code>brutetree</code> isn&#39;t actually a tree.</li><li><code>reorder::Bool = true</code> : if <code>true</code> then points which are close in    distance are placed close in memory. In this case, a copy of the original data    will be made so that the original data is left unmodified. Setting this to <code>true</code>    can significantly improve performance of the specified <code>algorithm</code>    (except <code>:brutetree</code>). This option is ignored and always taken as <code>false</code> for    <code>algorithm = :brutetree</code>.</li><li><code>weights::KNNKernel=Uniform()</code> : kernel used in assigning weights to the    k-nearest neighbors for each observation. An instance of one of the types in    <code>list_kernels()</code>. User-defined weighting functions can be passed by wrapping the    function in a <a href="#NearestNeighborModels.UserDefinedKernel"><code>UserDefinedKernel</code></a> kernel (do <code>?NearestNeighborModels.UserDefinedKernel</code> for more    info). If observation weights <code>w</code> are passed during machine construction then the    weight assigned to each neighbor vote is the product of the kernel generated    weight for that neighbor and the corresponding observation weight.</li></ul><p><strong>Operations</strong></p><ul><li><code>predict(mach, Xnew)</code>: Return predictions of the target given features <code>Xnew</code>, which should have same scitype as <code>X</code> above.</li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>tree</code>: An instance of either <code>KDTree</code>, <code>BruteTree</code> or <code>BallTree</code> depending on the  value of the <code>algorithm</code> hyperparameter (See hyper-parameters section above).  These are data structures that stores the training data with the view of making  quicker nearest neighbor searches on test data points.</li></ul><p><strong>Examples</strong></p><pre><code class="language-none">using MLJ

# Create Data
X, y = make_regression(10, 5, n_targets=2)

# load MultitargetKNNRegressor
MultitargetKNNRegressor = @load MultitargetKNNRegressor pkg=NearestNeighborModels

# view possible kernels
NearestNeighborModels.list_kernels()

# MutlitargetKNNRegressor instantiation
model = MultitargetKNNRegressor(weights = NearestNeighborModels.Inverse())

# Wrap model and required data in an MLJ machine and fit.
mach = machine(model, X, y) |&gt; fit! 

# Predict
y_hat = predict(mach, X)
</code></pre><p>See also <a href="#NearestNeighborModels.KNNRegressor"><code>KNNRegressor</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/NearestNeighborModels.jl/blob/0664bab3ab3e6a09a10af344c745a7da48c2a977/src/models.jl#LL756-L800">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NearestNeighborModels.MultitargetKNNClassifier" href="#NearestNeighborModels.MultitargetKNNClassifier"><code>NearestNeighborModels.MultitargetKNNClassifier</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MultitargetKNNClassifier</code></pre><p>A model type for constructing a multitarget K-nearest neighbor classifier, based on <a href="https://github.com/JuliaAI/NearestNeighborModels.jl">NearestNeighborModels.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="language-none">MultitargetKNNClassifier = @load MultitargetKNNClassifier pkg=NearestNeighborModels</code></pre><p>Do <code>model = MultitargetKNNClassifier()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>MultitargetKNNClassifier(K=...)</code>.</p><p>Multi-target K-Nearest Neighbors Classifier (MultitargetKNNClassifier) is a variation of  <a href="#NearestNeighborModels.KNNClassifier"><code>KNNClassifier</code></a> that assumes the target variable is vector-valued with <code>Multiclass</code> or <code>OrderedFactor</code> components. (Target data must be presented as a table, however.)</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with</p><pre><code class="language-none">mach = machine(model, X, y)</code></pre><p>OR</p><pre><code class="language-none">mach = machine(model, X, y, w)</code></pre><p>Here:</p><ul><li><p><code>X</code> is any table of input features (eg, a <code>DataFrame</code>) whose columns are of scitype <code>Continuous</code>; check column scitypes with <code>schema(X)</code>.</p></li><li><p>y<code>is the target, which can be any table of responses whose element scitype is either</code>&lt;:Finite<code>(</code>&lt;:Multiclass<code>or</code>&lt;:OrderedFactor<code>will do); check the columns scitypes with</code>schema(y)<code>.  Each column of</code>y` is assumed to belong to a common categorical pool.  </p></li><li><p><code>w</code> is the observation weights which can either be <code>nothing</code>(default) or an  <code>AbstractVector</code> whose element scitype is <code>Count</code> or <code>Continuous</code>. This is different  from <code>weights</code> kernel which is a model hyperparameter, see below.</p></li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Hyper-parameters</strong></p><ul><li><code>K::Int=5</code> : number of neighbors</li><li><code>algorithm::Symbol = :kdtree</code> : one of <code>(:kdtree, :brutetree, :balltree)</code></li><li><code>metric::Metric = Euclidean()</code> : any <code>Metric</code> from    <a href="https://github.com/JuliaStats/Distances.jl">Distances.jl</a> for the    distance between points. For <code>algorithm = :kdtree</code> only metrics which are    instances of <code>Union{Distances.Chebyshev, Distances.Cityblock, Distances.Euclidean, Distances.Minkowski, Distances.WeightedCityblock, Distances.WeightedEuclidean, Distances.WeightedMinkowski}</code> are supported.</li><li><code>leafsize::Int = algorithm == 10</code> : determines the number of points    at which to stop splitting the tree. This option is ignored and always taken as <code>0</code>    for <code>algorithm = :brutetree</code>, since <code>brutetree</code> isn&#39;t actually a tree.</li><li><code>reorder::Bool = true</code> : if <code>true</code> then points which are close in    distance are placed close in memory. In this case, a copy of the original data    will be made so that the original data is left unmodified. Setting this to <code>true</code>    can significantly improve performance of the specified <code>algorithm</code>    (except <code>:brutetree</code>). This option is ignored and always taken as <code>false</code> for    <code>algorithm = :brutetree</code>.</li><li><code>weights::KNNKernel=Uniform()</code> : kernel used in assigning weights to the    k-nearest neighbors for each observation. An instance of one of the types in    <code>list_kernels()</code>. User-defined weighting functions can be passed by wrapping the    function in a <a href="#NearestNeighborModels.UserDefinedKernel"><code>UserDefinedKernel</code></a> kernel (do <code>?NearestNeighborModels.UserDefinedKernel</code> for more    info). If observation weights <code>w</code> are passed during machine construction then the    weight assigned to each neighbor vote is the product of the kernel generated    weight for that neighbor and the corresponding observation weight.</li></ul><ul><li><code>output_type::Type{&lt;:MultiUnivariateFinite}=DictTable</code> : One of    (<code>ColumnTable</code>, <code>DictTable</code>). The type of table type to use for predictions.   Setting to <code>ColumnTable</code> might improve performance for narrow tables while setting to    <code>DictTable</code> improves performance for wide tables.</li></ul><p><strong>Operations</strong></p><ul><li><p><code>predict(mach, Xnew)</code>: Return predictions of the target given features <code>Xnew</code>, which should have same scitype as <code>X</code> above. Predictions are either a <code>ColumnTable</code> or  <code>DictTable</code> of <code>UnivariateFiniteVector</code> columns depending on the value set for the  <code>output_type</code> parameter discussed above. The probabilistic predictions are uncalibrated. </p></li><li><p><code>predict_mode(mach, Xnew)</code>: Return the modes of each column of the table of probabilistic  predictions returned above.</p></li></ul><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>tree</code>: An instance of either <code>KDTree</code>, <code>BruteTree</code> or <code>BallTree</code> depending on the  value of the <code>algorithm</code> hyperparameter (See hyper-parameters section above).  These are data structures that stores the training data with the view of making  quicker nearest neighbor searches on test data points.</li></ul><p><strong>Examples</strong></p><pre><code class="language-none">using MLJ, StableRNGs

# set rng for reproducibility
rng = StableRNG(10)

# Dataset generation
n, p = 10, 3
X = table(randn(rng, n, p)) # feature table
fruit, color = categorical([&quot;apple&quot;, &quot;orange&quot;]), categorical([&quot;blue&quot;, &quot;green&quot;])
y = [(fruit = rand(rng, fruit), color = rand(rng, color)) for _ in 1:n] # target_table
# Each column in y has a common categorical pool as expected
selectcols(y, :fruit) # categorical array
selectcols(y, :color) # categorical array

# Load MultitargetKNNClassifier
MultitargetKNNClassifier = @load MultitargetKNNClassifier pkg=NearestNeighborModels

# view possible kernels
NearestNeighborModels.list_kernels()

# MultitargetKNNClassifier instantiation
model = MultitargetKNNClassifier(K=3, weights = NearestNeighborModels.Inverse())

# wrap model and required data in an MLJ machine and fit
mach = machine(model, X, y) |&gt; fit!

# predict
y_hat = predict(mach, X)
labels = predict_mode(mach, X)
</code></pre><p>See also <a href="#NearestNeighborModels.KNNClassifier"><code>KNNClassifier</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/NearestNeighborModels.jl/blob/0664bab3ab3e6a09a10af344c745a7da48c2a977/src/models.jl#LL605-L649">source</a></section></article><h1 id="Kernels-1"><a class="docs-heading-anchor" href="#Kernels-1">Kernels</a><a class="docs-heading-anchor-permalink" href="#Kernels-1" title="Permalink"></a></h1><article class="docstring"><header><a class="docstring-binding" id="NearestNeighborModels.KNNKernel" href="#NearestNeighborModels.KNNKernel"><code>NearestNeighborModels.KNNKernel</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">KNNKernel</code></pre><p>Abstract super type for all weighting kernels</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/NearestNeighborModels.jl/blob/0664bab3ab3e6a09a10af344c745a7da48c2a977/src/kernels.jl#LL1-L4">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NearestNeighborModels.list_kernels" href="#NearestNeighborModels.list_kernels"><code>NearestNeighborModels.list_kernels</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">list_kernels()</code></pre><p>Lists all implemented KNN weighting kernels</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/NearestNeighborModels.jl/blob/0664bab3ab3e6a09a10af344c745a7da48c2a977/src/kernels.jl#LL9-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NearestNeighborModels.DualU" href="#NearestNeighborModels.DualU"><code>NearestNeighborModels.DualU</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">DualU()</code></pre><p>Assigns the closest neighbor a weight of <code>1</code>, the furthest neighbor weight <code>0</code> and the  others are scaled between by a mapping.</p><p>For more information see the paper by Geler et.al <a href="https://perun.pmf.uns.ac.rs/ radovanovic/publications/2016-kais-knn-weighting.pdf">Comparison of different weighting  schemes for the kNN classifier on time-series data</a>.</p><p>see also: <a href="#NearestNeighborModels.DualD"><code>DualD</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/NearestNeighborModels.jl/blob/0664bab3ab3e6a09a10af344c745a7da48c2a977/src/kernels.jl#LL330-L343">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NearestNeighborModels.DualD" href="#NearestNeighborModels.DualD"><code>NearestNeighborModels.DualD</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">DualD()</code></pre><p>Assigns the closest neighbor a weight of <code>1</code>, the furthest neighbor weight <code>0</code> and the  others are scaled between by a mapping.</p><p>For more information see the paper by Geler et.al <a href="https://perun.pmf.uns.ac.rs/ radovanovic/publications/2016-kais-knn-weighting.pdf">Comparison of different weighting  schemes for the kNN classifier on time-series data</a>.</p><p>see also: <a href="#NearestNeighborModels.DualU"><code>DualU</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/NearestNeighborModels.jl/blob/0664bab3ab3e6a09a10af344c745a7da48c2a977/src/kernels.jl#LL365-L378">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NearestNeighborModels.Dudani" href="#NearestNeighborModels.Dudani"><code>NearestNeighborModels.Dudani</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Dudani()</code></pre><p>Assigns the closest neighbor a weight of <code>1</code>, the furthest neighbor weight <code>0</code> and the  others are scaled between by a linear mapping.</p><p>For more information see the paper by Geler et.al <a href="https://perun.pmf.uns.ac.rs/ radovanovic/publications/2016-kais-knn-weighting.pdf">Comparison of different weighting  schemes for the kNN classifier on time-series data</a>.</p><p>see also: <a href="#NearestNeighborModels.Macleod"><code>Macleod</code></a>, <a href="#NearestNeighborModels.DualU"><code>DualU</code></a>, <a href="#NearestNeighborModels.DualD"><code>DualD</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/NearestNeighborModels.jl/blob/0664bab3ab3e6a09a10af344c745a7da48c2a977/src/kernels.jl#LL96-L108">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NearestNeighborModels.Fibonacci" href="#NearestNeighborModels.Fibonacci"><code>NearestNeighborModels.Fibonacci</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Fibonacci()</code></pre><p>Assigns neighbors weights corresponding to fibonacci numbers starting from the furthest neighbor. i.e the furthest neighbor a weight of <code>1</code>, the second furthest neighbor a  weight of <code>1</code> and the third furthest neighbor a weight of <code>2</code> and so on.  </p><p>For more information see the paper by Geler et.al <a href="https://perun.pmf.uns.ac.rs/ radovanovic/publications/2016-kais-knn-weighting.pdf">Comparison of different weighting  schemes for the kNN classifier on time-series data</a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/NearestNeighborModels.jl/blob/0664bab3ab3e6a09a10af344c745a7da48c2a977/src/kernels.jl#LL401-L412">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NearestNeighborModels.Inverse" href="#NearestNeighborModels.Inverse"><code>NearestNeighborModels.Inverse</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Inverse()</code></pre><p>Assigns each neighbor a weight equal to the inverse of the corresponsting distance of the  neighbor.</p><p>For more information see the paper by Geler et.al <a href="https://perun.pmf.uns.ac.rs/ radovanovic/publications/2016-kais-knn-weighting.pdf">Comparison of different weighting  schemes for the kNN classifier on time-series data</a>.</p><p>see also: <a href="#NearestNeighborModels.ISquared"><code>ISquared</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/NearestNeighborModels.jl/blob/0664bab3ab3e6a09a10af344c745a7da48c2a977/src/kernels.jl#LL130-L142">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NearestNeighborModels.ISquared" href="#NearestNeighborModels.ISquared"><code>NearestNeighborModels.ISquared</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ISquared()</code></pre><p>Assigns each neighbor a weight equal to the inverse of the corresponsting squared-distance  of the neighbor.</p><p>For more information see the paper by Geler et.al <a href="https://perun.pmf.uns.ac.rs/ radovanovic/publications/2016-kais-knn-weighting.pdf">Comparison of different weighting  schemes for the kNN classifier on time-series data</a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/NearestNeighborModels.jl/blob/0664bab3ab3e6a09a10af344c745a7da48c2a977/src/kernels.jl#LL170-L180">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NearestNeighborModels.Macleod" href="#NearestNeighborModels.Macleod"><code>NearestNeighborModels.Macleod</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Macleod(;a::Real= 0.0)</code></pre><p>Assigns the closest neighbor a weight of <code>1</code>, the furthest neighbor weight <code>0</code> and the  others are scaled between by a linear mapping.</p><p>For more information see the paper by Geler et.al <a href="https://perun.pmf.uns.ac.rs/ radovanovic/publications/2016-kais-knn-weighting.pdf">Comparison of different weighting  schemes for the kNN classifier on time-series data</a>.</p><p>see also: <a href="#NearestNeighborModels.Dudani"><code>Dudani</code></a>, <a href="#NearestNeighborModels.DualU"><code>DualU</code></a>, <a href="#NearestNeighborModels.DualD"><code>DualD</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/NearestNeighborModels.jl/blob/0664bab3ab3e6a09a10af344c745a7da48c2a977/src/kernels.jl#LL230-L243">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NearestNeighborModels.Rank" href="#NearestNeighborModels.Rank"><code>NearestNeighborModels.Rank</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Rank()</code></pre><p>Assigns each neighbor a weight as a rank such that the closest neighbor get&#39;s a weight of  <code>1</code> and the Kth closest neighbor gets a weight of <code>K</code>.</p><p>For more information see the paper by Geler et.al <a href="https://perun.pmf.uns.ac.rs/ radovanovic/publications/2016-kais-knn-weighting.pdf">Comparison of different weighting  schemes for the kNN classifier on time-series data</a>.</p><p>see also: <a href="#NearestNeighborModels.ReciprocalRank"><code>ReciprocalRank</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/NearestNeighborModels.jl/blob/0664bab3ab3e6a09a10af344c745a7da48c2a977/src/kernels.jl#LL199-L211">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NearestNeighborModels.ReciprocalRank" href="#NearestNeighborModels.ReciprocalRank"><code>NearestNeighborModels.ReciprocalRank</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ReciprocalRank(;a::Real= 0.0)</code></pre><p>Assigns each closest neighbor a weight which is equal to the reciprocal of it&#39;s rank.  i.e the closest neighbor get&#39;s a weight of <code>1</code> and the Kth closest weight get&#39;s a weight  of <code>1/K</code> </p><p>For more information see the paper by Geler et.al <a href="https://perun.pmf.uns.ac.rs/ radovanovic/publications/2016-kais-knn-weighting.pdf">Comparison of different weighting  schemes for the kNN classifier on time-series data</a>.</p><p>see also: <a href="#NearestNeighborModels.Rank"><code>Rank</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/NearestNeighborModels.jl/blob/0664bab3ab3e6a09a10af344c745a7da48c2a977/src/kernels.jl#LL302-L316">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NearestNeighborModels.UDK" href="#NearestNeighborModels.UDK"><code>NearestNeighborModels.UDK</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">UDK</code></pre><p>Alias for <code>UserDefinedKernel</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/NearestNeighborModels.jl/blob/0664bab3ab3e6a09a10af344c745a7da48c2a977/src/kernels.jl#LL46-L49">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NearestNeighborModels.Uniform" href="#NearestNeighborModels.Uniform"><code>NearestNeighborModels.Uniform</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Uniform()</code></pre><p>assigns uniform weights to all k-nearest neighbors.</p><p>see also: <a href="#NearestNeighborModels.Inverse"><code>Inverse</code></a>, <a href="#NearestNeighborModels.ISquared"><code>ISquared</code></a>, <a href="#NearestNeighborModels.Zavreal"><code>Zavreal</code></a>     </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/NearestNeighborModels.jl/blob/0664bab3ab3e6a09a10af344c745a7da48c2a977/src/kernels.jl#LL79-L86">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NearestNeighborModels.UserDefinedKernel" href="#NearestNeighborModels.UserDefinedKernel"><code>NearestNeighborModels.UserDefinedKernel</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">UserDefinedKernel(;func::Function = x-&gt;nothing, sort::Bool=false)</code></pre><p>Wrap a user defined nearest neighbors weighting function <code>func</code> as a <code>KNNKernel</code>.</p><p><strong>Keywords</strong></p><ul><li><code>func</code> : user-defined nearest neighbors weighting function. The function   should have the signature <code>func(dists_matrix)::Union{Nothing, &lt;:AbstractMatrix}</code>.  The <code>dists_matrix</code> is a <code>n</code> by <code>K</code> nearest neighbors distances matrix where   <code>n</code> is the number of samples in the test dataset and <code>K</code> is number of neighbors.   <code>func</code> should either output <code>nothing</code> or an <code>AbstractMatrix</code> of the same shape   as <code>dists_matrix</code>.  If <code>func(dists_matrix)</code> returns nothing then all k-nearest   neighbors in each row are assign equal weights.</li><li><code>sort</code> : if true requests that the <code>dists_matrix</code> be sorted before being passed   to <code>func</code>. The sort is done in a manner that puts the k-nearest neighbors in   each row  of <code>dists_matrix</code> in acesending order .</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/NearestNeighborModels.jl/blob/0664bab3ab3e6a09a10af344c745a7da48c2a977/src/kernels.jl#LL21-L39">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NearestNeighborModels.Zavreal" href="#NearestNeighborModels.Zavreal"><code>NearestNeighborModels.Zavreal</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Zavreal(;s::Real = 0.0, a::Real=1.0)</code></pre><p>Assigns each neighbor an exponential weight given by     <span>$e^{ - α ⋅ d_i^{eta}}$</span> where <code>α</code> and <code>β</code> are constants and <code>dᵢ</code> is the distance of the given neighbor.</p><p>For more information see the paper by Geler et.al <a href="https://perun.pmf.uns.ac.rs/ radovanovic/publications/2016-kais-knn-weighting.pdf">Comparison of different weighting  schemes for the kNN classifier on time-series data</a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/NearestNeighborModels.jl/blob/0664bab3ab3e6a09a10af344c745a7da48c2a977/src/kernels.jl#LL274-L285">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 15 May 2023 19:56">Monday 15 May 2023</span>. Using Julia version 1.9.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
